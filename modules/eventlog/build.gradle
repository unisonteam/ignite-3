/*
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

plugins {
    id "me.champeau.jmh" version "0.7.2"
    id "io.morethan.jmhreport" version "0.9.6"
}

apply from: "$rootDir/buildscripts/java-core.gradle"
apply from: "$rootDir/buildscripts/publishing.gradle"
apply from: "$rootDir/buildscripts/java-junit5.gradle"
apply from: "$rootDir/buildscripts/java-integration-test.gradle"

description = 'ignite-eventlog'

dependencies {
    annotationProcessor project(':ignite-configuration-annotation-processor')
    annotationProcessor libs.auto.service

    implementation project(':ignite-api')
    implementation project(':ignite-core')
    implementation project(':ignite-configuration-api')
    implementation project(':ignite-configuration-root')
    implementation libs.jackson.core
    implementation libs.jackson.databind
    implementation libs.auto.service.annotations

    testImplementation libs.hamcrest.core
    testImplementation libs.hamcrest.json
    testImplementation libs.awaitility
    testImplementation testFixtures(project(':ignite-core'))
    testImplementation testFixtures(project(':ignite-configuration'))
    testImplementation project(':ignite-configuration')

    integrationTestAnnotationProcessor project(':ignite-configuration-annotation-processor')

    integrationTestImplementation libs.awaitility
    integrationTestImplementation testFixtures(project(':ignite-runner'))
    integrationTestImplementation testFixtures(project(':ignite-core'))
    integrationTestImplementation testFixtures(project(':ignite-configuration'))
    integrationTestImplementation project(':ignite-api')
    integrationTestImplementation project(':ignite-configuration')
    integrationTestImplementation project(':ignite-configuration-root')
    integrationTestImplementation project(':ignite-client')
}

test {
    systemProperty "buildDirPath", project.buildDir.path
}

integrationTest {
    systemProperty "buildDirPath", project.buildDir.path
}


tasks.register("runnerPlatformBenchmark", JavaExec) {
    mainClass = "org.apache.ignite.internal.runner.app.PlatformBenchmarkNodeRunner"

    jvmArgs += defaultJvmArgs

    classpath = sourceSets.integrationTest.runtimeClasspath

    enableAssertions = true
}


jmh {
    iterations = 4 // Number of measurement iterations to do.
    benchmarkMode = [
            'all'
    ] // Benchmark mode. Available modes are: [Throughput/thrpt, AverageTime/avgt, SampleTime/sample, SingleShotTime/ss, All/all]
    //batchSize = 1 // Batch size: number of benchmark method calls per operation. (some benchmark modes can ignore this setting)
    fork = 0 // How many times to forks a single benchmark. Use 0 to disable forking altogether
    failOnError = true // Should JMH fail immediately if any benchmark had experienced the unrecoverable error?
    forceGC = false // Should JMH force GC between iterations?
    humanOutputFile = project.file("${project.buildDir}/reports/jmh/human.txt") // human-readable output file
    resultsFile = project.file("${project.buildDir}/reports/jmh/results.json") // results file
    operationsPerInvocation = 1 // Operations per invocation.
    benchmarkParameters =  [:] // Benchmark parameters.
    profilers = ["async:libPath=${project.projectDir}/src/libasyncProfiler.dylib;output=flamegraph"] // Use profilers to collect additional data. Supported profilers: [cl, comp, gc, stack, perf, perfnorm, perfasm, xperf, xperfasm, hs_cl, hs_comp, hs_gc, hs_rt, hs_thr, async]
    timeOnIteration = '1s' // Time to spend at each measurement iteration.
    resultFormat = 'JSON' // Result format type (one of CSV, JSON, NONE, SCSV, TEXT)
    synchronizeIterations = false // Synchronize iterations?
    threads = 1 // Number of worker threads to run with.
    //threadGroups = [2,3,4] //Override thread group distribution for asymmetric benchmarks.
    jmhTimeout = '5s' // Timeout for benchmark iteration.
    timeUnit = 's' // Output time unit. Available time units are: [m, s, ms, us, ns].
    verbosity = 'NORMAL' // Verbosity mode. Available modes are: [SILENT, NORMAL, EXTRA]
    warmup = '5s' // Time to spend at each warmup iteration.
    warmupBatchSize = 10 // Warmup batch size: number of benchmark method calls per operation.
    warmupForks = 0 // How many warmup forks to make for a single benchmark. 0 to disable warmup forks.
    warmupIterations = 1 // Number of warmup iterations to do.
    //warmupMode = 'INDI' // Warmup mode for warming up selected benchmarks. Warmup modes are: [INDI, BULK, BULK_INDI].
    //warmupBenchmarks = ['.*Warmup'] // Warmup benchmarks to include in the run in addition to already selected. JMH will not measure these benchmarks, but only use them for the warmup.

    //zip64 = true // Use ZIP64 format for bigger archives
    //jmhVersion = '1.37' // Specifies JMH version
    //includeTests = true // Allows to include test sources into generate JMH jar, i.e. use it when benchmarks depend on the test classes.
    duplicateClassesStrategy = DuplicatesStrategy.FAIL // Strategy to apply when encountring duplicate classes during creation of the fat jar (i.e. while executing jmhJar task)
}

jmhReport {
    jmhResultPath = project.file("${project.buildDir}/reports/jmh/results.json")
    jmhReportOutput = project.file("${project.buildDir}/reports/jmh")
}

tasks.jmh.finalizedBy tasks.jmhReport
